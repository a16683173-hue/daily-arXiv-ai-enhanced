{"id": "2602.22654", "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache\uff1a\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u89c4\u5212\u9009\u62e9\u5173\u952e\u65f6\u95f4\u6b65\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6548\u679c\u597d\uff0c\u4f46\u591a\u6b65\u8fed\u4ee3\u91c7\u6837\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6216\u5c40\u90e8\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u4e0d\u8003\u8651\u53bb\u566a\u8f68\u8ff9\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "\u5c06\u6269\u6563\u91c7\u6837\u52a0\u901f\u5efa\u6a21\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u6784\u5efa\u8def\u5f84\u611f\u77e5\u6210\u672c\u5f20\u91cf\u91cf\u5316\u8df3\u8fc7\u65f6\u95f4\u6b65\u7684\u8def\u5f84\u76f8\u5173\u8bef\u5dee\uff0c\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u9009\u62e9\u6700\u5c0f\u5316\u603b\u8def\u5f84\u6210\u672c\u7684\u5173\u952e\u65f6\u95f4\u6b65\u5e8f\u5217\u3002\u63a8\u7406\u65f6\u53ea\u5728\u5173\u952e\u65f6\u95f4\u6b65\u8fdb\u884c\u5b8c\u6574\u8ba1\u7b97\uff0c\u4e2d\u95f4\u8f93\u51fa\u4f7f\u7528\u7f13\u5b58\u7279\u5f81\u9884\u6d4b\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cDPCache\u57284.87\u500d\u52a0\u901f\u4e0bImageReward\u63d0\u5347+0.031\uff0c\u57283.54\u500d\u52a0\u901f\u4e0b\u751a\u81f3\u8d85\u8fc7\u5168\u6b65\u57fa\u7ebf+0.028 ImageReward\uff0c\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "DPCache\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u52a0\u901f\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u52a0\u901f\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22745", "pdf": "https://arxiv.org/pdf/2602.22745", "abs": "https://arxiv.org/abs/2602.22745", "authors": ["Fengming Liu", "Tat-Jen Cham", "Chuanxia Zheng"], "title": "SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.", "AI": {"tldr": "SPATIALALIGN\uff1a\u4e00\u4e2a\u901a\u8fc7\u96f6\u9636\u6b63\u5219\u5316DPO\u5fae\u8c03T2V\u6a21\u578b\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u4e2d\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u5bf9\u9f50\u80fd\u529b", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u8fc7\u4e8e\u5173\u6ce8\u7f8e\u5b66\u8d28\u91cf\uff0c\u4f46\u7ecf\u5e38\u5ffd\u89c6\u751f\u6210\u89c6\u9891\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u8868\u8fbe\u6587\u672c\u63d0\u793a\u4e2d\u6307\u5b9a\u7684\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb", "method": "\u63d0\u51faSPATIALALIGN\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u96f6\u9636\u6b63\u5219\u5316\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5fae\u8c03T2V\u6a21\u578b\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e\u51e0\u4f55\u7684DSR-SCORE\u8bc4\u4f30\u6307\u6807\uff1b3\uff09\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u6587\u672c-\u89c6\u9891\u5bf9\u6570\u636e\u96c6", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0cDSR-SCORE\u6307\u6807\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edfVLM\u8bc4\u4f30\u66f4\u7cbe\u786e\u7684\u5b9a\u91cf\u6d4b\u91cf", "conclusion": "SPATIALALIGN\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86T2V\u6a21\u578b\u5bf9\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u7a7a\u95f4\u7ea6\u675f\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.22960", "pdf": "https://arxiv.org/pdf/2602.22960", "abs": "https://arxiv.org/abs/2602.22960", "authors": ["Tianxing Xu", "Zixuan Wang", "Guangyuan Wang", "Li Hu", "Zhongyi Zhang", "Peng Zhang", "Bang Zhang", "Song-Hai Zhang"], "title": "UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models", "categories": ["cs.CV"], "comment": "Project Page: https://humanaigc.github.io/ucm-webpage/", "summary": "World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.", "AI": {"tldr": "UCM\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u626d\u66f2\u673a\u5236\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u7cbe\u786e\u76f8\u673a\u63a7\u5236\uff0c\u4f7f\u7528\u53cc\u6d41\u6269\u6563transformer\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u751f\u6210\uff0c\u5728\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u548c\u76f8\u673a\u53ef\u63a7\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u7684\u4e16\u754c\u6a21\u578b\u5728\u957f\u671f\u5185\u5bb9\u4e00\u81f4\u6027\uff08\u573a\u666f\u91cd\u8bbf\u65f6\uff09\u548c\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u73b0\u6709\u57fa\u4e8e\u663e\u5f0f3D\u91cd\u5efa\u7684\u65b9\u6cd5\u5728\u65e0\u754c\u573a\u666f\u548c\u7ec6\u7c92\u5ea6\u7ed3\u6784\u4e0a\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u800c\u4f9d\u8d56\u5148\u524d\u751f\u6210\u5e27\u7684\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faUCM\u6846\u67b6\uff1a1\uff09\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u626d\u66f2\u673a\u5236\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u76f8\u673a\u63a7\u5236\uff1b2\uff09\u9ad8\u6548\u53cc\u6d41\u6269\u6563transformer\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff1b3\uff09\u57fa\u4e8e\u70b9\u4e91\u6e32\u67d3\u7684\u53ef\u6269\u5c55\u6570\u636e\u7b56\u5c55\u7b56\u7565\uff0c\u6a21\u62df\u573a\u666f\u91cd\u8bbf\uff0c\u572850\u4e07+\u5355\u76ee\u89c6\u9891\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCM\u5728\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u76f8\u673a\u53ef\u63a7\u6027\u3002", "conclusion": "UCM\u901a\u8fc7\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u76f8\u673a\u63a7\u5236\u7684\u65b0\u9896\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u7684\u4e16\u754c\u6a21\u578b\u5728\u573a\u666f\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u3002"}}
{"id": "2602.23141", "pdf": "https://arxiv.org/pdf/2602.23141", "abs": "https://arxiv.org/abs/2602.23141", "authors": ["Tao Liu", "Gang Wan", "Kan Ren", "Shibo Wen"], "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors", "categories": ["cs.CV"], "comment": "CVPR2026", "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u89c6\u9891\u7a33\u5b9a\u6846\u67b6\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u7ecf\u5178\u6d41\u7a0b\u548c\u591a\u7ebf\u7a0b\u7f13\u51b2\u673a\u5236\uff0c\u5728UAV\u591c\u95f4\u9065\u611f\u7b49\u65b0\u9886\u57df\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u914d\u5bf9\u6570\u636e\u96c6\u3001\u53ef\u63a7\u6027\u5dee\u3001\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u89c6\u9891\u7a33\u5b9a\u5230UAV\u591c\u95f4\u9065\u611f\u7b49\u65b0\u5e94\u7528\u9886\u57df", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7ecf\u5178\u7a33\u5b9a\u5316\u6d41\u7a0b\uff08\u8fd0\u52a8\u4f30\u8ba1\u3001\u8fd0\u52a8\u5e73\u6ed1\u3001\u56fe\u50cf\u626d\u66f2\uff09\uff0c\u7ed3\u5408\u591a\u7ebf\u7a0b\u7f13\u51b2\u673a\u5236\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u5b8c\u5168\u65e0\u76d1\u7763", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u7a33\u5b9a\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u79bb\u7ebf\u65b9\u6cd5\uff0c\u5728\u65b0\u63d0\u51fa\u7684UAV-Test\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u89c6\u9891\u7a33\u5b9a\u6846\u67b6\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u9ad8\u6548\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230UAV\u591c\u95f4\u9065\u611f\u7b49\u65b0\u5e94\u7528\u573a\u666f"}}
{"id": "2602.23191", "pdf": "https://arxiv.org/pdf/2602.23191", "abs": "https://arxiv.org/abs/2602.23191", "authors": ["Xinyuan Chen", "Yao Xu", "Shaowen Wang", "Pengjie Song", "Bowen Deng"], "title": "Uni-Animator: Towards Unified Visual Colorization", "categories": ["cs.CV"], "comment": "10 pages, 8 figures. Submitted to CVPR 2026", "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.", "AI": {"tldr": "Uni-Animator\uff1a\u57fa\u4e8eDiffusion Transformer\u7684\u7edf\u4e00\u56fe\u50cf\u4e0e\u89c6\u9891\u8349\u56fe\u7740\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u53c2\u8003\u589e\u5f3a\u3001\u7269\u7406\u7ec6\u8282\u5f3a\u5316\u548c\u52a8\u6001RoPE\u7f16\u7801\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u5927\u95ee\u9898", "motivation": "\u73b0\u6709\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\uff0c\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5355/\u591a\u53c2\u8003\u4e0b\u7684\u989c\u8272\u4f20\u9012\u4e0d\u7cbe\u786e\uff1b2\uff09\u9ad8\u9891\u7269\u7406\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\uff1b3\uff09\u5927\u8fd0\u52a8\u573a\u666f\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\u4e14\u6709\u8fd0\u52a8\u4f2a\u5f71", "method": "1\uff09\u901a\u8fc7\u5b9e\u4f8b\u8865\u4e01\u5d4c\u5165\u5b9e\u73b0\u89c6\u89c9\u53c2\u8003\u589e\u5f3a\uff0c\u7cbe\u786e\u5bf9\u9f50\u548c\u878d\u5408\u53c2\u8003\u989c\u8272\u4fe1\u606f\uff1b2\uff09\u4f7f\u7528\u7269\u7406\u7279\u5f81\u8fdb\u884c\u7269\u7406\u7ec6\u8282\u5f3a\u5316\uff0c\u6709\u6548\u6355\u6349\u548c\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\uff1b3\uff09\u63d0\u51fa\u57fa\u4e8e\u8349\u56fe\u7684\u52a8\u6001RoPE\u7f16\u7801\uff0c\u81ea\u9002\u5e94\u5efa\u6a21\u8fd0\u52a8\u611f\u77e5\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUni-Animator\u5728\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e0e\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u5907\u7edf\u4e00\u7684\u8de8\u57df\u80fd\u529b\uff0c\u5177\u6709\u9ad8\u7ec6\u8282\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u7684\u65f6\u95f4\u4e00\u81f4\u6027", "conclusion": "Uni-Animator\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u7684\u4e09\u5927\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\uff0c\u5728\u989c\u8272\u4f20\u9012\u7cbe\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.23203", "pdf": "https://arxiv.org/pdf/2602.23203", "abs": "https://arxiv.org/abs/2602.23203", "authors": ["Junhu Fu", "Shuyu Liang", "Wutong Li", "Chen Ma", "Peng Huang", "Kehao Wang", "Ke Chen", "Shengli Lin", "Pinghong Zhou", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.", "AI": {"tldr": "ColoDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6d41\u6a21\u5757\u548c\u5185\u5bb9\u611f\u77e5\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u5c5e\u6027\u7cbe\u786e\u63a7\u5236\uff0c\u80fd\u591f\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u80a0\u955c\u89c6\u9891\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u8bca\u65ad\u80a0\u9053\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u9762\u4e34\u80a0\u9053\u7ed3\u6784\u4e0d\u89c4\u5219\u3001\u75be\u75c5\u8868\u73b0\u591a\u6837\u3001\u6210\u50cf\u6a21\u5f0f\u5404\u5f02\u7b49\u6311\u6218\uff0c\u9700\u8981\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u63d0\u51faColoDiff\u6846\u67b6\uff1a1) TimeStream\u6a21\u5757\u901a\u8fc7\u8de8\u5e27\u6807\u8bb0\u5316\u673a\u5236\u89e3\u8026\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5b9e\u73b0\u590d\u6742\u52a8\u6001\u5efa\u6a21\uff1b2) Content-Aware\u6a21\u5757\u7ed3\u5408\u566a\u58f0\u6ce8\u5165\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u539f\u578b\uff0c\u5b9e\u73b0\u4e34\u5e8a\u5c5e\u6027\u7cbe\u786e\u63a7\u5236\uff1b3) \u91c7\u7528\u975e\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u7b56\u7565\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u6b65\u9aa4\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u533b\u9662\u6570\u636e\u5e93\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8e\u751f\u6210\u6307\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u75be\u75c5\u8bca\u65ad\u3001\u6a21\u6001\u5224\u522b\u3001\u80a0\u9053\u51c6\u5907\u8bc4\u5206\u3001\u75c5\u53d8\u5206\u5272\uff09\u5747\u8868\u73b0\u4f18\u5f02\u3002\u5b9e\u9a8c\u663e\u793aColoDiff\u80fd\u751f\u6210\u5177\u6709\u5e73\u6ed1\u8fc7\u6e21\u548c\u4e30\u5bcc\u52a8\u6001\u7684\u89c6\u9891\u3002", "conclusion": "ColoDiff\u5c55\u793a\u4e86\u53ef\u63a7\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u7684\u6f5c\u529b\uff0c\u63ed\u793a\u4e86\u5408\u6210\u89c6\u9891\u5728\u8865\u5145\u771f\u5b9e\u8868\u793a\u548c\u7f13\u89e3\u4e34\u5e8a\u73af\u5883\u4e2d\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u7684\u4ef7\u503c\uff0c\u4e3a\u4e34\u5e8a\u5206\u6790\u63d0\u4f9b\u8f85\u52a9\u3002"}}
{"id": "2602.23262", "pdf": "https://arxiv.org/pdf/2602.23262", "abs": "https://arxiv.org/abs/2602.23262", "authors": ["Jasmine Bayrooti", "Weiwei Kong", "Natalia Ponomareva", "Carlos Esteves", "Ameesh Makadia", "Amanda Prorok"], "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u8c31\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\uff1a\u7b2c\u4e00\u9636\u6bb5\u5bf9\u4f4e\u9891\u5c0f\u6ce2\u7cfb\u6570\u8fdb\u884cDP\u5fae\u8c03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u516c\u5f00\u9884\u8bad\u7ec3\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8fdb\u884c\u4e0a\u91c7\u6837\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u654f\u611f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u5b58\u5728\u8bb0\u5fc6\u548c\u91cd\u73b0\u8bad\u7ec3\u6837\u672c\u7684\u98ce\u9669\uff0c\u9700\u8981\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002\u4f20\u7edf\u7684\u5dee\u5206\u9690\u79c1\u5fae\u8c03\uff08\u5982DP-SGD\uff09\u901a\u5e38\u4f1a\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u4e25\u91cd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9891\u7eb9\u7406\u65b9\u9762\uff0c\u56e0\u4e3a\u566a\u58f0\u88ab\u4e0d\u52a0\u533a\u5206\u5730\u6dfb\u52a0\u5230\u6240\u6709\u6a21\u578b\u53c2\u6570\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u8c31\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u5047\u8bbe\u56fe\u50cf\u4e2d\u6700\u9690\u79c1\u654f\u611f\u7684\u90e8\u5206\u901a\u5e38\u662f\u4f4e\u9891\u5206\u91cf\uff08\u5982\u9762\u90e8\u7279\u5f81\u548c\u7269\u4f53\u5f62\u72b6\uff09\uff0c\u800c\u9ad8\u9891\u5206\u91cf\u5927\u591a\u662f\u901a\u7528\u548c\u516c\u5f00\u7684\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5bf9\u654f\u611f\u56fe\u50cf\u7684\u4f4e\u5206\u8fa8\u7387\u5c0f\u6ce2\u7cfb\u6570\u8fdb\u884cDP\u5fae\u8c03\u81ea\u56de\u5f52\u8c31\u56fe\u50cf\u6807\u8bb0\u5668\u6a21\u578b\uff1b2\uff09\u4f7f\u7528\u516c\u5f00\u9884\u8bad\u7ec3\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u4e0a\u91c7\u6837\u3002", "result": "\u5728MS-COCO\u548cMM-CelebA-HQ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5176\u4ed6\u9886\u5148\u7684DP\u56fe\u50cf\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u6539\u8fdb\u8d28\u91cf\u548c\u98ce\u683c\u6355\u6349\u80fd\u529b\u7684\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u826f\u597d\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9690\u79c1\u9884\u7b97\u9650\u5236\u5728\u56fe\u50cf\u5168\u5c40\u7ed3\u6784\u7684\u7b2c\u4e00\u9636\u6bb5\uff0c\u5e76\u5229\u7528DP\u7684\u540e\u5904\u7406\u7279\u6027\u8fdb\u884c\u7ec6\u8282\u7ec6\u5316\uff0c\u8be5\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u5e73\u8861\uff0c\u4e3aDP\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.22974", "pdf": "https://arxiv.org/pdf/2602.22974", "abs": "https://arxiv.org/abs/2602.22974", "authors": ["L. Martino", "M. M. Garcia", "P. S. Paradas", "E. Curbelo"], "title": "An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets", "categories": ["cs.CE", "cs.CV", "eess.IV", "eess.SP", "stat.ML"], "comment": null, "summary": "Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8ba1\u6570\u5927\u9f20\u810a\u9ad3\u5207\u7247\u4e2d\u5fae\u80f6\u8d28\u7ec6\u80de\u7684\u6838\u65b9\u6cd5\uff0c\u65e0\u9700\u7ec6\u80de\u68c0\u6d4b\uff0c\u76f4\u63a5\u8fdb\u884c\u8ba1\u6570\u4efb\u52a1\uff0c\u652f\u6301\u5c0f\u6570\u636e\u96c6\u8bad\u7ec3\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u624b\u52a8\u8ba1\u6570\u7ec6\u80de\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u57f9\u8bad\uff0c\u4f20\u7edf\u81ea\u52a8\u989c\u8272\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u6807\u8bb0\u533a\u57df\u548c\u5f3a\u5ea6\u4fe1\u606f\uff0c\u65e0\u6cd5\u51c6\u786e\u8ba1\u6570\u7ec6\u80de\u3002\u56fe\u50cf\u5206\u8fa8\u7387\u9ad8\u4f46\u5305\u542b\u5927\u91cf\u566a\u58f0\u548c\u4f2a\u5f71\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8ba1\u6570\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u8fdb\u884c\u9884\u5904\u7406\u751f\u6210\u591a\u4e2a\u6ee4\u6ce2\u56fe\u50cf\uff08\u63d0\u4f9b\u5b9a\u5236\u5316\u7279\u5f81\u63d0\u53d6\uff09\uff0c\u7136\u540e\u8bbe\u8ba1\u975e\u53c2\u6570\u3001\u975e\u7ebf\u6027\u7684\u6838\u8ba1\u6570\u5668\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u53ef\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u540c\u65f6\u80fd\u5904\u7406\u4e30\u5bcc\u5f02\u6784\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u4eba\u5de5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u51fa\u975e\u5e38\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u76f8\u5173Matlab\u4ee3\u7801\u4e5f\u5df2\u63d0\u4f9b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6838\u8ba1\u6570\u5668\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5fae\u80f6\u8d28\u7ec6\u80de\u8ba1\u6570\u95ee\u9898\uff0c\u907f\u514d\u4e86\u7ec6\u80de\u68c0\u6d4b\u7684\u590d\u6742\u6027\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
