{"id": "2602.22960", "pdf": "https://arxiv.org/pdf/2602.22960", "abs": "https://arxiv.org/abs/2602.22960", "authors": ["Tianxing Xu", "Zixuan Wang", "Guangyuan Wang", "Li Hu", "Zhongyi Zhang", "Peng Zhang", "Bang Zhang", "Song-Hai Zhang"], "title": "UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models", "categories": ["cs.CV"], "comment": "Project Page: https://humanaigc.github.io/ucm-webpage/", "summary": "World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.", "AI": {"tldr": "UCM\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u626d\u66f2\u673a\u5236\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u7cbe\u786e\u76f8\u673a\u63a7\u5236\uff0c\u5728\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u4e2d\u89e3\u51b3\u4e86\u573a\u666f\u91cd\u8bbf\u65f6\u7684\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u7528\u6237\u8f93\u5165\u76f8\u673a\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u5728\u957f\u671f\u5185\u5bb9\u4e00\u81f4\u6027\uff08\u573a\u666f\u91cd\u8bbf\u65f6\uff09\u548c\u7cbe\u786e\u76f8\u673a\u63a7\u5236\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u57fa\u4e8e\u663e\u5f0f3D\u91cd\u5efa\u7684\u65b9\u6cd5\u5728\u65e0\u754c\u573a\u666f\u548c\u7ec6\u7c92\u5ea6\u7ed3\u6784\u4e0a\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u57fa\u4e8e\u5148\u524d\u751f\u6210\u5e27\u7684\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faUCM\u6846\u67b6\uff1a1\uff09\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u626d\u66f2\u673a\u5236\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u76f8\u673a\u63a7\u5236\uff1b2\uff09\u9ad8\u6548\u53cc\u6d41\u6269\u6563transformer\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff1b3\uff09\u57fa\u4e8e\u70b9\u4e91\u6e32\u67d3\u7684\u53ef\u6269\u5c55\u6570\u636e\u7b56\u5c55\u7b56\u7565\uff0c\u572850\u4e07+\u5355\u76ee\u89c6\u9891\u4e0a\u8bad\u7ec3\u573a\u666f\u91cd\u8bbf\u6a21\u62df\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCM\u5728\u957f\u671f\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u76f8\u673a\u53ef\u63a7\u6027\u3002", "conclusion": "UCM\u901a\u8fc7\u7edf\u4e00\u957f\u671f\u8bb0\u5fc6\u548c\u76f8\u673a\u63a7\u5236\u7684\u521b\u65b0\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23141", "pdf": "https://arxiv.org/pdf/2602.23141", "abs": "https://arxiv.org/abs/2602.23141", "authors": ["Tao Liu", "Gang Wan", "Kan Ren", "Shibo Wen"], "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors", "categories": ["cs.CV"], "comment": "CVPR2026", "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u89c6\u9891\u7a33\u5b9a\u6846\u67b6\uff0c\u91c7\u7528\u7ecf\u5178\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf+\u591a\u7ebf\u7a0b\u7f13\u51b2\u673a\u5236\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u4e09\u5927\u6311\u6218\uff0c\u5e76\u5728\u65b0\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u914d\u5bf9\u7a33\u5b9a/\u4e0d\u7a33\u5b9a\u6570\u636e\u96c6\uff0c\u5b58\u5728\u4e09\u5927\u95ee\u9898\uff1a\u6570\u636e\u6709\u9650\u3001\u53ef\u63a7\u6027\u5dee\u3001\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u6548\u7387\u4f4e\u3002\u6b64\u5916\uff0c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u624b\u6301\u53ef\u89c1\u5149\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5728\u65e0\u4eba\u673a\u591c\u95f4\u9065\u611f\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7ecf\u5178\u7a33\u5b9a\u5316\u6d41\u6c34\u7ebf\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff08\u7279\u5f81\u63d0\u53d6\u3001\u8fd0\u52a8\u4f30\u8ba1\u3001\u5e73\u6ed1\uff09\uff0c\u7ed3\u5408\u591a\u7ebf\u7a0b\u7f13\u51b2\u673a\u5236\u3002\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u662f\u65e0\u76d1\u7763\u65b9\u6cd5\u3002\u8fd8\u5f15\u5165\u4e86\u65b0\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u822a\u7a7a\u89c6\u9891\u6570\u636e\u96c6(UAV-Test)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u7a33\u5b9a\u5668\uff0c\u6027\u80fd\u53ef\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u5ab2\u7f8e\u3002\u5728\u65b0\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u89c6\u9891\u7a33\u5b9a\u6846\u67b6\u89e3\u51b3\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u4e09\u5927\u6311\u6218\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u6269\u5c55\u4e86\u7a33\u5b9a\u5316\u6280\u672f\u5230\u65e0\u4eba\u673a\u9065\u611f\u7b49\u65b0\u9886\u57df\u3002"}}
{"id": "2602.23203", "pdf": "https://arxiv.org/pdf/2602.23203", "abs": "https://arxiv.org/abs/2602.23203", "authors": ["Junhu Fu", "Shuyu Liang", "Wutong Li", "Chen Ma", "Peng Huang", "Kehao Wang", "Ke Chen", "Shengli Lin", "Pinghong Zhou", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.", "AI": {"tldr": "ColoDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6d41\u6a21\u5757\u548c\u5185\u5bb9\u611f\u77e5\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u5c5e\u6027\u7cbe\u786e\u63a7\u5236\uff0c\u80fd\u51cf\u5c1190%\u4ee5\u4e0a\u91c7\u6837\u6b65\u9aa4\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u8f85\u52a9\u4e34\u5e8a\u5206\u6790\u3002", "motivation": "\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u8bca\u65ad\u80a0\u9053\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u9762\u4e34\u80a0\u9053\u7ed3\u6784\u4e0d\u89c4\u5219\u3001\u75be\u75c5\u8868\u73b0\u591a\u6837\u548c\u6210\u50cf\u6a21\u5f0f\u5404\u5f02\u7b49\u6311\u6218\uff0c\u9700\u8981\u5b9e\u73b0\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u4e34\u5e8a\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u63d0\u51faColoDiff\u6846\u67b6\uff1a1) TimeStream\u6a21\u5757\u901a\u8fc7\u8de8\u5e27\u6807\u8bb0\u5316\u673a\u5236\u89e3\u8026\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5b9e\u73b0\u590d\u6742\u52a8\u6001\u5efa\u6a21\uff1b2) Content-Aware\u6a21\u5757\u7ed3\u5408\u566a\u58f0\u6ce8\u5165\u5d4c\u5165\u548c\u53ef\u5b66\u4e60\u539f\u578b\uff0c\u5b9e\u73b0\u4e34\u5e8a\u5c5e\u6027\u7cbe\u786e\u63a7\u5236\uff1b3) \u91c7\u7528\u975e\u9a6c\u5c14\u53ef\u592b\u91c7\u6837\u7b56\u7565\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u6b65\u9aa4\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u533b\u9662\u6570\u636e\u5e93\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8e\u751f\u6210\u6307\u6807\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u75be\u75c5\u8bca\u65ad\u3001\u6a21\u6001\u5224\u522b\u3001\u80a0\u9053\u51c6\u5907\u8bc4\u5206\u3001\u75c5\u53d8\u5206\u5272\uff09\u3002\u5b9e\u9a8c\u8868\u660eColoDiff\u80fd\u751f\u6210\u5177\u6709\u5e73\u6ed1\u8fc7\u6e21\u548c\u4e30\u5bcc\u52a8\u6001\u7684\u89c6\u9891\u3002", "conclusion": "ColoDiff\u5c55\u793a\u4e86\u53ef\u63a7\u7ed3\u80a0\u955c\u89c6\u9891\u751f\u6210\u7684\u6f5c\u529b\uff0c\u63ed\u793a\u4e86\u5408\u6210\u89c6\u9891\u5728\u8865\u5145\u771f\u5b9e\u8868\u793a\u548c\u7f13\u89e3\u4e34\u5e8a\u73af\u5883\u4e2d\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
