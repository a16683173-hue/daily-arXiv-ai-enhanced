<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: UCM框架通过时间感知位置编码扭曲机制统一长期记忆和精确相机控制，在视频生成世界模型中解决了场景重访时的内容一致性和用户输入相机控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成世界模型在长期内容一致性（场景重访时）和精确相机控制方面存在困难。基于显式3D重建的方法在无界场景和细粒度结构上缺乏灵活性，而基于先前生成帧的方法缺乏显式空间对应关系，限制了可控性和一致性。

Method: 提出UCM框架：1）时间感知位置编码扭曲机制统一长期记忆和相机控制；2）高效双流扩散transformer减少计算开销；3）基于点云渲染的可扩展数据策展策略，在50万+单目视频上训练场景重访模拟。

Result: 在真实世界和合成基准测试中，UCM在长期场景一致性方面显著优于最先进方法，同时在高质量视频生成中实现了精确的相机可控性。

Conclusion: UCM通过统一长期记忆和相机控制的创新机制，解决了视频生成世界模型中的关键挑战，为交互环境模拟提供了更强大和可控的解决方案。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [2] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出新的无监督在线视频稳定框架，采用经典三阶段流水线+多线程缓冲机制，无需配对数据集，解决端到端学习的三大挑战，并在新无人机数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法需要配对稳定/不稳定数据集，存在三大问题：数据有限、可控性差、资源受限硬件效率低。此外，现有基准主要针对手持可见光视频，限制了在无人机夜间遥感等领域的应用。

Method: 采用经典稳定化流水线的三阶段框架（特征提取、运动估计、平滑），结合多线程缓冲机制。无需配对训练数据，是无监督方法。还引入了新的多模态无人机航空视频数据集(UAV-Test)。

Result: 实验表明，该方法在定量指标和视觉质量上均优于现有在线稳定器，性能可与离线方法媲美。在新无人机数据集上表现优异。

Conclusion: 提出的无监督在线视频稳定框架解决了端到端学习的三大挑战，无需配对数据集，在资源受限硬件上高效运行，并扩展了稳定化技术到无人机遥感等新领域。

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [3] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: ColoDiff是一个基于扩散模型的结肠镜视频生成框架，通过时间流模块和内容感知模块实现动态一致性和临床属性精确控制，能减少90%以上采样步骤，在数据稀缺场景中辅助临床分析。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频生成对于诊断肠道疾病至关重要，特别是在数据稀缺的情况下。然而，高质量视频生成面临肠道结构不规则、疾病表现多样和成像模式各异等挑战，需要实现时间一致性和临床属性的精确控制。

Method: 提出ColoDiff框架：1) TimeStream模块通过跨帧标记化机制解耦时间依赖性，实现复杂动态建模；2) Content-Aware模块结合噪声注入嵌入和可学习原型，实现临床属性精确控制；3) 采用非马尔可夫采样策略，减少90%以上步骤实现实时生成。

Result: 在三个公共数据集和一个医院数据库上评估，基于生成指标和下游任务（疾病诊断、模态判别、肠道准备评分、病变分割）。实验表明ColoDiff能生成具有平滑过渡和丰富动态的视频。

Conclusion: ColoDiff展示了可控结肠镜视频生成的潜力，揭示了合成视频在补充真实表示和缓解临床环境中数据稀缺方面的价值。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>


### [4] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: UCM框架通过时间感知位置编码扭曲机制统一长期记忆和精确相机控制，使用双流扩散Transformer进行高效高保真生成，在500K单目视频上训练，显著提升场景一致性和相机可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频生成的世界模型在长期内容一致性（场景重访时）和精确相机控制方面存在困难。基于显式3D重建的方法在无界场景和细粒度结构上灵活性不足，而依赖先前生成帧的方法缺乏明确空间对应关系，限制了可控性和一致性。

Method: 提出UCM框架：1）时间感知位置编码扭曲机制统一长期记忆和精确相机控制；2）高效双流扩散Transformer减少计算开销；3）基于点云渲染的可扩展数据策展策略模拟场景重访，在500K+单目视频上训练。

Result: 在真实世界和合成基准测试中，UCM在长期场景一致性方面显著优于最先进方法，同时在高质量视频生成中实现精确的相机可控性。

Conclusion: UCM通过统一长期记忆和相机控制的创新机制，解决了视频生成世界模型中的关键挑战，在场景一致性和可控性方面取得了显著进展。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [5] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出新的无监督在线视频稳定框架，采用经典三阶段流程+多线程缓冲机制，无需配对数据集，解决端到端学习的三大挑战，并在新无人机数据集上超越现有在线方法


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法需要配对数据集，存在数据有限、可控性差、资源受限硬件效率低三大问题。现有基准测试主要针对手持可见光视频，限制了在无人机夜间遥感等领域的应用。

Method: 采用经典三阶段稳定化流程，结合多线程缓冲机制的无监督框架，无需配对稳定/不稳定数据集。引入新的多模态无人机航拍视频数据集(UAV-Test)进行验证。

Result: 在定量指标和视觉质量上持续超越最先进的在线稳定器，性能可与离线方法媲美。在新无人机数据集上验证了方法的有效性。

Conclusion: 提出的无监督在线视频稳定框架解决了端到端学习的三大挑战，扩展了稳定化技术到无人机夜间遥感等新领域，性能优于现有在线方法。

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [6] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: ColoDiff是一个基于扩散模型的结肠镜视频生成框架，通过时间流模块和内容感知模块实现动态一致性和临床属性精确控制，可减少90%以上采样步骤，在数据稀缺场景下辅助临床分析。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频生成对于诊断肠道疾病至关重要，特别是在数据稀缺场景下。然而，高质量视频生成面临肠道结构不规则、疾病表现多样、成像模态多变等挑战，需要实现时间一致性和临床属性的精确控制。

Method: 提出ColoDiff框架：1) 时间流模块通过跨帧标记化机制解耦时间依赖性，实现复杂动态建模；2) 内容感知模块结合噪声注入嵌入和可学习原型，实现临床属性精确控制；3) 采用非马尔可夫采样策略，减少90%以上步骤实现实时生成。

Result: 在三个公共数据集和一个医院数据库上评估，基于生成指标和下游任务（疾病诊断、模态判别、肠道准备评分、病变分割）均表现优异。实验显示ColoDiff能生成具有平滑过渡和丰富动态的视频。

Conclusion: ColoDiff展示了可控结肠镜视频生成的潜力，揭示了合成视频在补充真实表示和缓解临床环境中数据稀缺方面的应用前景。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>


### [7] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache：一种基于动态规划的扩散模型加速框架，通过全局路径规划选择关键时间步，在保持生成质量的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成效果好，但多步迭代采样计算开销大。现有缓存加速方法采用固定或局部自适应调度，忽略了去噪轨迹的全局结构，导致误差累积和视觉伪影。

Method: 将扩散采样加速建模为全局路径规划问题。构建路径感知成本张量量化跳过时间步的误差，使用动态规划选择最小化总路径成本的关键时间步序列。推理时只在关键时间步进行完整计算，中间输出通过缓存特征预测。

Result: 在DiT、FLUX和HunyuanVideo上实验表明，DPCache在4.87倍加速下获得+0.031 ImageReward提升，在3.54倍加速下甚至超过全步基线+0.028 ImageReward，优于现有加速方法。

Conclusion: DPCache通过全局路径规划有效解决了扩散模型加速中的误差累积问题，实现了高质量加速，为扩散模型的实际部署提供了有效的训练免费解决方案。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [8] [SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation](https://arxiv.org/abs/2602.22745)
*Fengming Liu,Tat-Jen Cham,Chuanxia Zheng*

Main category: cs.CV

TL;DR: SPATIALALIGN：一个自改进框架，通过零阶正则化DPO微调文本到视频模型，提升动态空间关系（DSR）的生成能力，并设计了基于几何的DSR-SCORE评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频（T2V）生成器过于注重美学质量，而忽视了生成视频中的空间约束，无法准确表达文本提示中指定的动态空间关系。

Method: 提出SPATIALALIGN自改进框架，采用零阶正则化直接偏好优化（DPO）微调T2V模型；设计了基于几何的DSR-SCORE评估指标，并构建了包含多样动态空间关系的文本-视频对数据集。

Result: 实验表明，经过微调的模型在空间关系生成方面显著优于基线模型，能够更好地对齐文本提示中指定的动态空间关系。

Conclusion: SPATIALALIGN框架有效提升了T2V模型对动态空间关系的表达能力，DSR-SCORE指标为空间对齐评估提供了更可靠的几何基础方法。

Abstract: Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.

</details>


### [9] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: Uni-Animator是一个基于扩散Transformer的统一图像和视频草图着色框架，通过视觉参考增强、物理细节强化和动态RoPE编码解决现有方法在颜色转移、细节保留和时间一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有草图着色方法难以统一处理图像和视频任务，存在颜色转移不精确、高频物理细节保留不足、大运动场景中时间一致性差等问题。

Method: 提出基于扩散Transformer的统一框架，包含三个关键技术：1) 通过实例补丁嵌入实现视觉参考增强；2) 使用物理特征进行物理细节强化；3) 基于草图的动态RoPE编码来建模运动感知的时空依赖关系。

Result: 实验结果表明，Uni-Animator在图像和视频草图着色任务上均取得有竞争力的性能，与任务特定方法相当，同时实现了统一的跨域能力，具有高细节保真度和鲁棒的时间一致性。

Conclusion: Uni-Animator成功解决了现有草图着色方法的局限性，为图像和视频草图着色提供了一个统一、高性能的解决方案，在颜色转移精度、细节保留和时间一致性方面表现出色。

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [10] [Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling](https://arxiv.org/abs/2602.23262)
*Jasmine Bayrooti,Weiwei Kong,Natalia Ponomareva,Carlos Esteves,Ameesh Makadia,Amanda Prorok*

Main category: cs.CV

TL;DR: 提出基于小波变换的谱域差分隐私图像生成框架，通过将隐私预算集中在低分辨率小波系数（低频结构信息）上，结合公开预训练的超分辨率模型，在保证隐私的同时提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型在敏感图像数据集上训练时可能记忆并重现训练样本，需要强隐私保证。传统差分隐私微调（如DP-SGD）通常导致图像质量严重下降，特别是高频纹理，因为噪声被不加区分地添加到所有模型参数中。

Method: 提出两阶段谱域差分隐私框架：1）在敏感图像的低分辨率小波系数上差分隐私微调自回归谱图像分词器模型；2）使用公开预训练的超分辨率模型进行高分辨率上采样。假设隐私敏感信息主要存在于小波空间的低频分量（如面部特征和物体形状），而高频分量大多是通用和公开的。

Result: 在MS-COCO和MM-CelebA-HQ数据集上的实验表明，该方法相比其他领先的差分隐私图像框架，能生成质量更高、风格捕捉更好的图像，实现了隐私与效用的更好权衡。

Conclusion: 通过将隐私预算限制在图像全局结构（低频分量）上，并利用差分隐私的后处理特性进行细节细化，该方法在隐私保护和图像质量之间取得了有前景的平衡，为差分隐私图像生成提供了新思路。

Abstract: Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [11] [An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets](https://arxiv.org/abs/2602.22974)
*L. Martino,M. M. Garcia,P. S. Paradas,E. Curbelo*

Main category: cs.CE

TL;DR: 提出一种基于核方法的自动细胞计数方案，跳过细胞检测直接进行计数，适用于小数据集且能提供不确定性估计


<details>
  <summary>Details</summary>
Motivation: 传统手动细胞计数耗时且需要专业培训，现有自动方法只能提供标记面积和强度信息，无法准确计数细胞数量

Method: 先进行预处理生成多个过滤图像，然后设计非参数非线性核计数方法，只需一个超参数，能处理噪声和伪影

Result: 在人工和真实数据集上的实验显示非常理想的结果，能提供预测不确定性估计，并能处理多个专家意见

Conclusion: 提出的核计数方法是一种灵活有效的自动细胞计数方案，特别适合小数据集，并提供相关Matlab代码

Abstract: Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.

</details>
