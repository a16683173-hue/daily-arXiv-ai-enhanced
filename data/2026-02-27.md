<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: UCM框架通过时间感知位置编码扭曲机制统一长期记忆和精确相机控制，在视频生成世界模型中解决了场景重访时的内容一致性和用户输入相机控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成世界模型在长期内容一致性（场景重访时）和精确相机控制方面存在困难。基于显式3D重建的方法在无界场景和细粒度结构上缺乏灵活性，而基于先前生成帧的方法缺乏显式空间对应关系，限制了可控性和一致性。

Method: 提出UCM框架：1）时间感知位置编码扭曲机制统一长期记忆和相机控制；2）高效双流扩散transformer减少计算开销；3）基于点云渲染的可扩展数据策展策略，在50万+单目视频上训练场景重访模拟。

Result: 在真实世界和合成基准测试中，UCM在长期场景一致性方面显著优于最先进方法，同时在高质量视频生成中实现了精确的相机可控性。

Conclusion: UCM通过统一长期记忆和相机控制的创新机制，解决了视频生成世界模型中的关键挑战，为交互环境模拟提供了更强大和可控的解决方案。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [2] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出新的无监督在线视频稳定框架，采用经典三阶段流水线+多线程缓冲机制，无需配对数据集，解决端到端学习的三大挑战，并在新无人机数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法需要配对稳定/不稳定数据集，存在三大问题：数据有限、可控性差、资源受限硬件效率低。此外，现有基准主要针对手持可见光视频，限制了在无人机夜间遥感等领域的应用。

Method: 采用经典稳定化流水线的三阶段框架（特征提取、运动估计、平滑），结合多线程缓冲机制。无需配对训练数据，是无监督方法。还引入了新的多模态无人机航空视频数据集(UAV-Test)。

Result: 实验表明，该方法在定量指标和视觉质量上均优于现有在线稳定器，性能可与离线方法媲美。在新无人机数据集上表现优异。

Conclusion: 提出的无监督在线视频稳定框架解决了端到端学习的三大挑战，无需配对数据集，在资源受限硬件上高效运行，并扩展了稳定化技术到无人机遥感等新领域。

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [3] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: ColoDiff是一个基于扩散模型的结肠镜视频生成框架，通过时间流模块和内容感知模块实现动态一致性和临床属性精确控制，能减少90%以上采样步骤，在数据稀缺场景中辅助临床分析。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频生成对于诊断肠道疾病至关重要，特别是在数据稀缺的情况下。然而，高质量视频生成面临肠道结构不规则、疾病表现多样和成像模式各异等挑战，需要实现时间一致性和临床属性的精确控制。

Method: 提出ColoDiff框架：1) TimeStream模块通过跨帧标记化机制解耦时间依赖性，实现复杂动态建模；2) Content-Aware模块结合噪声注入嵌入和可学习原型，实现临床属性精确控制；3) 采用非马尔可夫采样策略，减少90%以上步骤实现实时生成。

Result: 在三个公共数据集和一个医院数据库上评估，基于生成指标和下游任务（疾病诊断、模态判别、肠道准备评分、病变分割）。实验表明ColoDiff能生成具有平滑过渡和丰富动态的视频。

Conclusion: ColoDiff展示了可控结肠镜视频生成的潜力，揭示了合成视频在补充真实表示和缓解临床环境中数据稀缺方面的价值。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>
