<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache：一种基于动态规划的扩散模型加速框架，通过全局路径规划选择关键时间步，在保持生成质量的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成效果好，但多步迭代采样计算开销大。现有缓存方法使用固定或局部自适应调度，不考虑去噪轨迹的全局结构，容易导致误差累积和视觉伪影。

Method: 将扩散采样加速建模为全局路径规划问题。构建路径感知成本张量量化跳过时间步的路径相关误差，使用动态规划选择最小化总路径成本的关键时间步序列。推理时只在关键时间步进行完整计算，中间输出使用缓存特征预测。

Result: 在DiT、FLUX和HunyuanVideo上实验显示，DPCache在4.87倍加速下ImageReward提升+0.031，在3.54倍加速下甚至超过全步基线+0.028 ImageReward，优于现有加速方法。

Conclusion: DPCache通过全局路径规划框架有效解决了扩散模型加速中的误差累积问题，实现了高质量加速，为扩散模型的实际部署提供了有效的训练免费解决方案。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [2] [SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation](https://arxiv.org/abs/2602.22745)
*Fengming Liu,Tat-Jen Cham,Chuanxia Zheng*

Main category: cs.CV

TL;DR: SPATIALALIGN：一个通过零阶正则化DPO微调T2V模型的自改进框架，显著提升视频生成中动态空间关系的对齐能力


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成器过于关注美学质量，但经常忽视生成视频中的空间约束，导致无法准确表达文本提示中指定的动态空间关系

Method: 提出SPATIALALIGN自改进框架，包含：1）零阶正则化直接偏好优化（DPO）微调T2V模型；2）设计基于几何的DSR-SCORE评估指标；3）构建包含多样化动态空间关系的文本-视频对数据集

Result: 微调后的模型在空间关系对齐方面显著优于基线模型，DSR-SCORE指标提供了比传统VLM评估更精确的定量测量

Conclusion: SPATIALALIGN框架有效提升了T2V模型对动态空间关系的理解和生成能力，为空间约束对齐提供了系统解决方案

Abstract: Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.

</details>


### [3] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: UCM框架通过时间感知位置编码扭曲机制统一长期记忆和精确相机控制，使用双流扩散transformer实现高效高保真生成，在长期场景一致性和相机可控性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于视频生成的世界模型在长期内容一致性（场景重访时）和精确相机控制方面存在困难。现有基于显式3D重建的方法在无界场景和细粒度结构上灵活性不足，而依赖先前生成帧的方法缺乏显式空间对应关系，限制了可控性和一致性。

Method: 提出UCM框架：1）时间感知位置编码扭曲机制统一长期记忆和相机控制；2）高效双流扩散transformer减少计算开销；3）基于点云渲染的可扩展数据策展策略，模拟场景重访，在50万+单目视频上训练。

Result: 在真实世界和合成基准测试中，UCM在长期场景一致性方面显著优于最先进方法，同时在高质量视频生成中实现精确的相机可控性。

Conclusion: UCM通过统一长期记忆和相机控制的新颖机制，解决了基于视频生成的世界模型在场景一致性和可控性方面的关键挑战，为交互环境模拟提供了更强大的框架。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [4] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出新的无监督在线视频稳定框架，无需配对数据集，采用三阶段经典流程和多线程缓冲机制，在UAV夜间遥感等新领域表现优异


<details>
  <summary>Details</summary>
Motivation: 解决现有基于深度学习方法需要配对数据集、可控性差、在资源受限硬件上效率低的问题，并扩展视频稳定到UAV夜间遥感等新应用领域

Method: 采用三阶段经典稳定化流程（运动估计、运动平滑、图像扭曲），结合多线程缓冲机制，无需配对训练数据，完全无监督

Result: 在定量指标和视觉质量上均优于现有在线稳定方法，性能接近离线方法，在新提出的UAV-Test多模态数据集上表现优异

Conclusion: 提出的无监督在线视频稳定框架解决了深度学习方法的局限性，在资源受限环境下表现高效，并成功扩展到UAV夜间遥感等新应用场景

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [5] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: Uni-Animator：基于Diffusion Transformer的统一图像与视频草图着色框架，通过视觉参考增强、物理细节强化和动态RoPE编码解决现有方法的三大问题


<details>
  <summary>Details</summary>
Motivation: 现有草图着色方法难以统一图像和视频任务，存在三个主要问题：1）单/多参考下的颜色传递不精确；2）高频物理细节保留不足；3）大运动场景中时间一致性差且有运动伪影

Method: 1）通过实例补丁嵌入实现视觉参考增强，精确对齐和融合参考颜色信息；2）使用物理特征进行物理细节强化，有效捕捉和保留高频纹理；3）提出基于草图的动态RoPE编码，自适应建模运动感知的时空依赖关系

Result: 实验结果表明，Uni-Animator在图像和视频草图着色任务上均取得有竞争力的性能，与任务特定方法相当，同时具备统一的跨域能力，具有高细节保真度和鲁棒的时间一致性

Conclusion: Uni-Animator成功解决了现有草图着色方法的三大挑战，实现了图像和视频任务的统一处理，在颜色传递精度、细节保留和时间一致性方面表现出色

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [6] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: ColoDiff是一个基于扩散模型的结肠镜视频生成框架，通过时间流模块和内容感知模块实现动态一致性和临床属性精确控制，能够快速生成高质量结肠镜视频以缓解数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频生成对于诊断肠道疾病至关重要，特别是在数据稀缺的情况下。然而，高质量视频生成面临肠道结构不规则、疾病表现多样、成像模式各异等挑战，需要实现时间一致性和临床属性的精确控制。

Method: 提出ColoDiff框架：1) TimeStream模块通过跨帧标记化机制解耦时间依赖性，实现复杂动态建模；2) Content-Aware模块结合噪声注入嵌入和可学习原型，实现临床属性精确控制；3) 采用非马尔可夫采样策略，减少90%以上步骤实现实时生成。

Result: 在三个公共数据集和一个医院数据库上评估，基于生成指标和下游任务（疾病诊断、模态判别、肠道准备评分、病变分割）均表现优异。实验显示ColoDiff能生成具有平滑过渡和丰富动态的视频。

Conclusion: ColoDiff展示了可控结肠镜视频生成的潜力，揭示了合成视频在补充真实表示和缓解临床环境中数据稀缺方面的价值，为临床分析提供辅助。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>


### [7] [Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling](https://arxiv.org/abs/2602.23262)
*Jasmine Bayrooti,Weiwei Kong,Natalia Ponomareva,Carlos Esteves,Ameesh Makadia,Amanda Prorok*

Main category: cs.CV

TL;DR: 提出基于小波变换的谱差分隐私框架，通过两阶段方法在保护隐私的同时提升生成图像质量：第一阶段对低频小波系数进行DP微调，第二阶段使用公开预训练的超分辨率模型进行上采样。


<details>
  <summary>Details</summary>
Motivation: 生成模型在敏感图像数据集上训练时存在记忆和重现训练样本的风险，需要强隐私保证。传统的差分隐私微调（如DP-SGD）通常会导致图像质量严重下降，特别是在高频纹理方面，因为噪声被不加区分地添加到所有模型参数中。

Method: 提出基于小波变换的谱差分隐私框架，假设图像中最隐私敏感的部分通常是低频分量（如面部特征和物体形状），而高频分量大多是通用和公开的。采用两阶段方法：1）对敏感图像的低分辨率小波系数进行DP微调自回归谱图像标记器模型；2）使用公开预训练的超分辨率模型进行高分辨率上采样。

Result: 在MS-COCO和MM-CelebA-HQ数据集上的实验表明，该方法相对于其他领先的DP图像框架，能够生成具有改进质量和风格捕捉能力的图像，实现了隐私与效用之间的良好权衡。

Conclusion: 通过将隐私预算限制在图像全局结构的第一阶段，并利用DP的后处理特性进行细节细化，该方法在隐私保护和图像质量之间取得了有前景的平衡，为DP图像生成提供了新的有效框架。

Abstract: Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [8] [An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets](https://arxiv.org/abs/2602.22974)
*L. Martino,M. M. Garcia,P. S. Paradas,E. Curbelo*

Main category: cs.CE

TL;DR: 提出一种用于计数大鼠脊髓切片中微胶质细胞的核方法，无需细胞检测，直接进行计数任务，支持小数据集训练并提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 手动计数细胞耗时且需要专业培训，传统自动颜色方法只能提供标记区域和强度信息，无法准确计数细胞。图像分辨率高但包含大量噪声和伪影，需要更精确的计数方法。

Method: 首先进行预处理生成多个滤波图像（提供定制化特征提取），然后设计非参数、非线性的核计数器。该方法仅依赖一个超参数，可在小数据集上训练，同时能处理丰富异构数据集，并提供预测不确定性估计。

Result: 在人工和真实数据集上的数值实验显示出非常有前景的结果，相关Matlab代码也已提供。

Conclusion: 提出的核计数器方法能够有效解决微胶质细胞计数问题，避免了细胞检测的复杂性，在小数据集上表现良好，并提供不确定性估计，具有实际应用价值。

Abstract: Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.

</details>
